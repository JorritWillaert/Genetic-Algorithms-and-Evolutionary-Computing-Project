\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage[dvinames]{xcolor}
\usepackage[compact,small]{titlesec}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{marginnote}
\usepackage[top=1.8cm, bottom=1.8cm, outer=1.8cm, inner=1.8cm, heightrounded, marginparwidth=2.5cm, marginparsep=0.5cm]{geometry}
\usepackage{enumitem}
\setlist{noitemsep,parsep=2pt}
\newcommand{\highlight}[1]{\textcolor{kuleuven}{#1}}
\usepackage{pythonhighlight}
\usepackage{cleveref}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\nextyear}{\advance\year by 1 \the\year\advance\year by -1}
\newcommand{\thisyear}{\the\year}
\newcommand{\deadlineCode}{December 31, \thisyear{} at 16:00 CET}
\newcommand{\deadlineReport}{\deadlineCode}

\newcommand{\ReplaceMe}[1]{{\color{blue}#1}}
\newcommand{\RemoveMe}[1]{{\color{purple}#1}}

\setlength{\parskip}{5pt}

%opening
\title{Evolutionary Algorithms: Final report}
\author{Jorrit Willaert (r0652971)}

\begin{document}
\fontfamily{ppl}
\selectfont{}

\maketitle

% TODO remove this section
% TODO change elimination operator somewhat?

\section{\RemoveMe{Formal requirements}} \label{sec_this}

\RemoveMe{The report is structured for fair and efficient grading of over 100 individual projects in the space of only a few days. Please respect the exact structure of this document. You are allowed to remove sections and . is the soul of wit: a good report will be \textbf{around $7.5$ pages} long. The hard limit is 12 pages. 

\begin{quote}
Think of this report as a \textbf{take-home exam}; it will be used at the exam for structuring the discussion and questions. Make an effort so that it can be visually scanned efficiently, e.g., by using boldface or colors to highlight key points, using lists, clearly defined paragraphs, figures, etc.

You do not need to explain in this report \textbf{how} the techniques and concepts that are literally in the slides work. The goal of this report is \textbf{not} to illustrate that you can reproduce the slides. You need to convince me that you aptly used these (and other) techniques in this project. If I have doubts about your understanding of certain concepts in the course materials, I will test this hypothesis at the exam.
\end{quote}

It is recommended that you use this \LaTeX{} template, but you are allowed to reproduce it with the same structure in a WYSIWYG-editor. The purple text containing our evaluation criteria can be removed. You should replace the blue text with your discussion.

This report should be uploaded to Toledo by \deadlineReport. It must be in the \textbf{Portable Document Format} (pdf) and must be named \texttt{r0123456\_final.pdf}, where r0123456 should be replaced with your student number.}

\section{Metadata}

\begin{itemize}
 \item \textbf{Group members during group phase:} Lukas De Greve and Thomas Vanhemel
 \item \textbf{Time spent on group phase:} \ReplaceMe{10 hours} % TODO
 \item \textbf{Time spent on final code:} \ReplaceMe{40 hours}  % TODO
 \item \textbf{Time spent on final report:} \ReplaceMe{10 hours} % TODO
\end{itemize}

\section{Peer review reports (target: 1 page)}

% TODO add some bold keywords

\RemoveMe{\textbf{Goal:} Based on this section, we will evaluate insofar as you are able to recognize and analyze common problems arising in the design and implementation of evolutionary algorithms and your ability to effectively solve them.}

\subsection{The weak points}
\ReplaceMe{List the (up to six distinct) weak points that were identified in the two peer review reports that you received. Use at most 3 sentences to describe each weak point.}

\begin{enumerate}
 \item Our initial recombination operator was a simplified version of the edge crossover operator \cite{initial_implementation_edge_crossover}. However, this version did not prioritize common edges between parents, but only chose an entry which itself had the shortest list. Hence, not enough exploitation of the parents features follows. 
 \item The ($\kappa$ + $\mu$)-elimination (without any type of diversity promotion) puts a lot of selective pressure on the population.
 \item Our mutation operator does not scale to larger problems, since it only swaps two random locations. As a consequence, the mutation operator will have a relatively even smaller impact on the solution when the problem size increases. 
 \item Due to the elimination strategy, together with the chosen mutation operator, premature convergence was observed.
\end{enumerate}

\subsection{The solutions}
\ReplaceMe{List the solutions to the identified weak points. Use at most 3 sentences to describe your solution. You do not need to explain the techniques in detail if they are in the slides or handbook; just state something like ``Weak point X was solved/mitigated/diminished by using the island model as diversity promotion scheme.'' Note there could be more or less solutions than the number of weak points.}

\begin{enumerate}
 \item The simplified version of the edge crossover operator was first replaced with the proper edge crossover operator \cite{eiben_smith}. However, once the running times of edge recombination were compared with the ones of order crossover, it was apparent that order crossover is much faster. For this reason, edge crossover was abandoned.
 \item The ($\kappa$ + $\mu$)-elimination was kept for quite some time, but was supplemented with fitness sharing. The high selective pressure of ($\kappa$ + $\mu$)-elimination was largely mitigated With the introduction of this diversity promotion scheme. However, to further reduce the selective pressure present in the ($\kappa$ + $\mu$)-elimination, the elimination eventually also used k-tournament (along with the same fitness sharing technique).
 \item The mutation operator has been changed from swap mutation to inversion mutation. Hence, the effect of the mutation operator is constant for rising problem sizes.
 \item With the introduction of fitness sharing, along with the inversion mutation operator, premature convergence was largely avoided.
\end{enumerate}

\ReplaceMe{Which weak points did you not address? List them and briefly motivate (target, 3 lines each).}

\begin{enumerate} % TODO replace this
 \item 
 \item 
\end{enumerate}

\subsection{The best suggestion}
\ReplaceMe{Among the two suggestions that you received and the two suggestions that you gave, which one did you find the most helpful? Briefly describe the suggestion and why you think it was the best. (suggested maximum: 10 lines)}

Both groups suggested to modify the simplified edge crossover operator to the `proper' one. Although this certainly is a useful suggestion, another suggestion by one group is even more useful in my opinion, given that I was going to change the crossover operator in the individual phase anyway. The other suggestion is about changing the mutation operator from the swap mutation to the inversion mutation. In the group phase part of the project, we hadn't really observed the shortcoming of the swap mutation, which became clear after their suggestion has been made.

\section{Changes since the group phase (target: $0.5$ pages)} 

\ReplaceMe{List the main changes that you implemented since the group phase. You do not need to explain the employed techniques in detail; for this, you should refer to the appropriate subsection of section 3 of the report. Naturally, there can be overlap with the solutions from the previous section.}

\begin{enumerate}
\item The simplified edge recombination operator was first replaced with the proper edge recombination operator, after which it eventually was replaced by the order recombination operator due to massive speed gains. An elaboration on the recombination operators is provided in Section \ref{recombination}). 
\item Fitness sharing has been introduced in the elimination step, as further explained in Section \ref{diversity_promotion}.
\item A local search operator (2-opt) has been introduced to increase the fitnesses of the newly created offsprings (Section \ref{local_search_operator}).
\item Simple random initialization has been replaced by greedy and legal initialization, as further explained in Section \ref{initialization}.
\item The mutation operator has been changed from the swap mutation to inversion mutation. This is more elaborated in Section \ref{mutation}.
\item The elimination operator has eventually been changed from ($\kappa$ + $\mu$)-elimination to k-tournament elimination, for reasons explained further in Section \ref{elimination}.
\item Numerous optimizations have been made in almost all parts of the algorithm to massively increase the execution speed.
\end{enumerate}

\section{Final design of the evolutionary algorithm (target: $3.5$ pages)} 

\RemoveMe{\textbf{Goal:} Based on this section, we will evaluate insofar as you are able to design and implement an advanced, effective evolutionary algorithm for solving a model problem.}


\subsection{The three main features}
\ReplaceMe{List the three main components of your evolutionary algorithm for this project. That is, what are its most distinctive characteristics, what components am I not allowed to change to a more basic version? Ideally these are some of the more advanced features that you added since the group phase.}

\begin{enumerate}
\item Fitness sharing has been used in the elimination step of the algorithm. This diversity promotion scheme is of crucial importance to avoid premature convergence, and hence makes sure that far better solutions can be found, instead of letting all individuals converge to one local minima.
\item By introducing the 2-opt local search operator, far better solutions were quickly found. Without the local search operator, much more iterations were required to find the same fitness values, along with a necessary larger population. Although this operator is inherently extremely computationally expensive, it is turns out to be pivotal in the algorithm. Especially in this operator, optimizations such as making use of dynamic programming and using Numba were decisive in making the operator computationally feasible.
\item One last crucial improvement is the introduction of the greedy and legally initializations. Greedy initialization starts from a random node, and chooses the next one according to the smallest distance. The details of this initialization scheme are elaborated in Section \ref{initialization}, along with a consideration of the introduced biases. Furthermore, legal initialization simply chooses the next node from a random neighbor that has an existing road between them.
\end{enumerate}

\subsection{The main loop}

5\ReplaceMe{Make a picture of the ``flow'' in your evolutionary algorithm, similar to the example below. Include all the main components (mutation, recombination, selection, elimination, initialization, local search operators, diversity promotion mechanisms). There are no formal requirements on how to do this, as long as it is clear and you can efficiently explain your complete evolutionary algorithm using this picture at the exam. Contrary to the picture below, include the specific techniques, e.g., top-$\lambda$ elimination, $k$-tournament selection, where possible.}

% TODO

%
%\begin{center}
%\includegraphics[height=10em]{../../BasicLoop.png}
%\end{center}

\clearpage
\RemoveMe{\textbf{The questions we ask from section 5.3 onwards in blue are there to guide which topics to discuss}, rather than an exact list of questions that must be answered. Feel free to add more items to discuss.}


\subsection{Representation}
Possible solutions are represented as \textbf{permutations} and are written down in \textbf{cycle notation}. E.g. the permutation (1423) starts at node 1, then goes to 4, then 2, then 3 and returns to 1. An advantage of this notation is that no cycles are present as long as we initialize the representations as a permutation of the list of all nodes. 

This representation is implemented in our program as a numpy array with length equal to the number of nodes in the problem. Each element in the array consists of one integer number: the node number.

\subsection{Initialization}
\label{initialization}
\ReplaceMe{How do you initialize the population? How did you determine the number of individuals? Did you implement advanced initialization mechanisms (local search operators, heuristic solutions)? If so, describe them. Do you believe your approach maintains sufficient diversity? How do you ensure that your population enrichment scheme does not immediately take over the population? Did you implement other initialization schemes that did not make it to the final version? Why did you discard them? How did you determine the population size?}
In the group-phase part of the projects, individuals were generated by a random permutation, with their size determined from the distance matrix. However, especially for the larger problem size, quite a lot of paths were nonexisting or extremely long. Hence, random initialization of all individuals yielded almost always individuals where non of them represented a valid path. 

Two new initialization schemes were introduced, legal and greedy initialization, where the objective of legal initialization is to not create impossible paths when initialing an individual. This while not introducing certain biases, such as individuals who take over the population immediately. The objective of greedy initialization is introducing locally optimal individuals with a high fitness, in a computationally inexpensive way. Here, special care has been taken so the individuals don't introduce high biases and won't take over the population immediately.

\subsubsection{Legal initialization}
When initializing an individual legally, one makes sure that the generated path exists. Therefore, a city is chosen at random, after which the next city in the tour is chosen to be a random one out of all the neighbors with a non-infinite path cost. If, however, no existing neighbors are available anymore as next cities, the whole procedure restarts. The pseudo-algorithm is given in Algorithm \ref{legal_initialization_algorithm}.

\begin{algorithm}
\caption{Legal initialization}\label{legal_initialization_algorithm}
\begin{algorithmic}
\While {True}
\State Choose the initial city at random
\While {individual not entirely generated}
\State Generate all legal possibilities
\If {no legal possibilities}
\State Break out of inner loop
\EndIf
\State Choose a random city to be the successor of the previous city
\EndWhile
\If {not broken out of inner loop and path from the last city to first one is not infinity}
\State Return newly generated individual
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

This legal initialization scheme does not introduce undesirable biases. In theory, also the most optimal route could have been initialized, and the only bias present is a bias to not have nonexisting paths. I therefore can't think of any disadvantage this initialization scheme entails. 

\subsubsection{Greedy initialization}
\label{greedy_initialization}
Besides the legal initialization scheme, I also initialize some individuals greedily. The algorithm resembles the legal initialization scheme, with the only change that the successor city is chosen to be the closest neighbor, instead of a random legal neighbor. The pseudo-algorithm is given in Algorithm \ref{greedy_initialization_algorithm}.

\begin{algorithm}
\caption{Greedy initialization}\label{greedy_initialization_algorithm}
\begin{algorithmic}
\While {True}
\State Choose the initial city at random
\While {individual not entirely generated}
\State Generate all legal possibilities
\If {no legal possibilities}
\State Break out of inner loop
\EndIf
\State Choose the closest neighboring city to be the successor of the previous city
\EndWhile
\If {not broken out of inner loop and path from the last city to first one is not infinity}
\State Return newly generated individual
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

This initialization scheme does introduce certain biases, which could result in some individuals taking over the population immediately. Therefore, one has to take preventions against this bias, by for example only initializing a fraction of all the individuals with this scheme, as could be observed in the chosen hyperparameters of Section \ref{general_aspects_initialization}.

The introduced bias is that all individuals are locally optimal, with in theory a maximum number of different solutions of the problem size. Given that it may be difficult to escape local minima, one must consider the usefulness of this initialization scheme.

However, after experimenting with this initialization scheme, it became apparent that it good solutions for the problem where found much faster, while still maintaining a lot of diversity, along with a smooth convergence. With a small portion of the individuals being initialized with this scheme, being totally stuck in a local minima was not observed, and for me it brought huge advantages, since the given five minutes could now be used to start searching in some way more interesting regions of the search space.

\subsubsection{General aspects}
\label{general_aspects_initialization}
The distance matrix can be constructed in a way where greedy initialization gets stuck in an infinite loop, because greedy initialization simply constructs a dead path, starting from each node. To create a legal path in special cases, it should instead sometimes take sub-optimal paths to a neighbor to not end up in a dead path near the end of the initialization. To prevent the whole algorithm from crashing, a time constraint on the initialization of one individual has been introduced. Once an individual takes longer than two second to initialize, simple random initialization of that individual ensues.

An individual also gets assigned a random $\alpha$ value, which represents the probability that the individual will mutate in the mutation step of the algorithm. This way, a suitable mutation rate is determined by self-adaptivity.

The initial value of $\alpha$ is given by Formula \ref{initial_alpha}.

\begin{equation}
    \label{initial_alpha}
    \alpha = max(0.01, 0.05+0.02 \cdot (X \sim \mathcal{N}(0,\,1)))
\end{equation}

After some testing with population sizes, a size of 15 % TODO still correct
has been chosen. Furthermore, as mentioned in Section \ref{greedy_initialization}, only a fraction of the population should be initialized greedily. Given that the  population size is 15 % TODO still correct
, I found that greedily initializing 20 \% of the individuals (i.e. 3 individuals) worked out quite well in practice. The remaining 80 \% of the individuals are initialized legally.

For large problem sizes, initialization could take up to 10 seconds. Since the initialization of an individual is totally independent of the other individuals, multiprocessing has been added to this step, which entailed a factor five speed improvement on a machine with four physical cores (eight virtual cores).


\subsection{Selection operators}
\label{selection}
\ReplaceMe{Which selection operators did you implement? If they are not from the slides, describe them. Can you motivate why you chose this one? Are there parameters that need to be chosen? Did you use an advanced scheme to vary these parameters throughout the iterations? Did you try other selection operators not included in the final version? Why did you discard them?}
The k-tournament selection operator from the group phase part has been kept. This selection operator is computationally inexpensive, since only k fitness values have to be computed, while this would require $\mu$ fitness values in fitness-based methods. Furthermore, sigma-scaled selection would for example not have been an appropriate choice, since the greedy initialization scheme introduces some very good individuals in the population. These individuals would dominate, since their selection probability would be very high.

A k-value of 4 % TODO still correct?
has been chosen after numerous experiments.

\subsection{Mutation operators}
\label{mutation}
\ReplaceMe{Which mutation operators did you implement? If they are not from the slides, describe them. How do you choose among several mutation operators? Do you believe it will introduce sufficient randomness? Can that be controlled with parameters? Do you use self-adaptivity? Do you use any other advanced parameter control mechanisms (e.g., variable across iterations)? Did you try other mutation operators not included in the final version? Why did you discard them?}
The mutation operator used for the final implementation is the inversion mutation, whereby a random sub-vector is chosen and its order is reversed. The swap mutation operator that was used in the group phase part, did not scale well to larger problems, since it only swaps two random locations. That mutation operator, as a consequence, had a relatively even smaller impact on the solution when the problem size increased.

Inversion mutation does not suffer from this scaling problem, since the cities that determine the sub-vector are randomly chosen. Hence, the effect of the mutation operator is constant for rising problem sizes.  

Self-adaptivity has been used for the mutation rate, which is hence specific to each individual, and initialized as ... % TODO
with a random variation of ... % TODO
.

\subsection{Recombination operators}
\label{recombination}
\ReplaceMe{Which recombination operators did you implement? If they are not from the slides, describe them. How do you choose among several recombination operators? Why did you choose these ones specifically? Explain how you believe that these operators can produce offspring that combine the best features from their parents. How does your operator behave if there is little overlap between the parents? Can your recombination be controlled with parameters; what behavior do they change? Do you use self-adaptivity? Do you use any other advanced parameter control mechanisms (e.g., variable across iterations)? Did you try other recombination operators not included in the final version? Why did you discard them? Did you consider recombination with arity strictly greater than 2?}

In the group phase part of the project, a simplified version of the edge crossover was used as the recombination algorithm. This process is described in Algorithm \ref{simple_edge_crossover} \cite{initial_implementation_edge_crossover}. This recombination results in a new path where all edges of the child were present in at least one of the parents. It does however not prioritize edges present in both parents over edges present in a single parent.

\begin{algorithm}
\caption{Simple edge recombination operator}\label{simple_edge_crossover}
\begin{algorithmic}
\State Let K be the empty list
\State let N be the first node of a random parent
\While{length(K) $<$ length(Parent)}
\State Append K to N
\State Remove N from all neighbor lists
\If{N's neighbor list is not empty}
\State let $N^*$ be the neighbor of N with the fewest neighbors in its list (or a random one, should there be multiple)
\Else 
\State let $N^*$ be a randomly chosen node that is not in K
\EndIf
\EndWhile
\State N $\gets N^*$
\end{algorithmic}
\end{algorithm}

This algorithm is very simple and was the weakest part of the genetic algorithm. However, the algorithm still has some desirable features despite its simplicity. When N consecutive edges are present in both parents (N $ \ge $ 2), the child will contain at least N/2 of these edges, so important features are mostly preserved. On the other hand,
when the parents are very different, the child will look fairly different from both parents. This combined with some aspects of randomness in the algorithm makes it decent at exploring the different solutions. 

The reason this simplified algorithm was implemented, instead of the proper one from Eiben \& Smith \cite{eiben_smith}, was due to the belief that the computational cost of this algorithm was (much) lower than the one from Eiben \& Smith. 

For the individual phase of the project, an analyses was made between order crossover and the proper edge crossover algorithm of Eiben \& Smith. After some research, with a lot of contradictory advices, an arbitrary choice has been made to first try out the proper edge crossover algorithm.

Implementation wise, quite a lot of effort has been made to catch all the corner cases of the algorithm, along with achieving relatively optimized code. The algorithm was kept a long time thereafter, until it was noticed that for the large problem sizes, crossover took an extremely long time (up to 95 \% of the total runtime was spend in the edge crossover operator).

\begin{algorithm}
\caption{`Proper' edge recombination operator \cite{eiben_smith}}\label{proper_edge_crossover}
\begin{algorithmic}
\State Construct the edge table
\State Pick an initial element at random and put it in the offspring
\While {the offspring is not entirely constructed}
\State Set the variable $current\_element = entry$
\State Remove all references to $current\_element$ from the table
\If {there is a common edge}
\State Pick that to be the next one
\Else
\State Pick the entry in the list which itself has the shortest list (split ties at random)
\EndIf
\If {an empty list is reached}
\If {the other end has not yet been examined}
\State Examine the other end for extension
\Else
\State Choose a new element at random
\EndIf
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}

Due to this slow execution time, order crossover \cite{eiben_smith} has been implemented as well (Algorithm \ref{order_crossover_algorithm}).

\begin{algorithm}
\caption{Order crossover operator \cite{eiben_smith}}\label{order_crossover_algorithm}
\begin{algorithmic}
\State Choose two crossover points at random
\State Copy the segment between them from the first parent into the offspring
\State Starting from the second crossover point in the second parent, copy the remaining unused numbers into the child in the order they appear in the second parent, wrapping around at the end of the list.
\end{algorithmic}
\end{algorithm}

This crossover algorithm is inherently much cheaper to calculate and takes only about 5 \% % TODO still correct?
of the total execution time in the final algorithm. This is exactly the reason why the crossover algorithm was eventually used. 

In hindsight, one reason for the slow execution time of the edge crossover operator is probably due to the usage of sets in the operator. The edge table was basically one list with sets, where a minus denoted a double entry. Sets were used because it was desirable to check quickly if a certain node was an edge in the edge table. However, since for each element, maximum four edges could be present, lists would probably have sufficed. Also because quite some bookkeeping was required with the sets (e.g. deleting a positive entry if it occurs for the second time, to insert it afterwards with a minus in front of it). 

Another reason for the big performance gap is the fact that the order crossover operator was able to use the Numba for compiling the Python code and running it way faster, by using the decorator $@jit(nopython=True)$. This because the order crossover operator only uses operations on Numpy arrays (which Numba handles perfectly well), while Numba threw hundreds of compile errors in the edge crossover method, because Numba (in the $nopython=True$ mode) couldn't create new Numpy arrays, had difficulties with working on sets, and wasn't able to infer $dtype$'s most of the time.

\subsection{Elimination operators}
\label{elimination}
\ReplaceMe{Which elimination operators did you implement? If they are not from the slides, describe them. Why did you select this one? Are there parameters that need to be chosen? Did you use an advanced scheme to vary these parameters throughout the iterations? Did you try other elimination operators not included in the final version? Why did you discard them?}
For a long time, the ($\kappa$ + $\mu$)-elimination from the group phase part was kept in the algorithm. However, for the smaller problem sizes, it was noted that the population converged extremely quickly, even with the fitness promotion scheme present (as further discussed in Section \ref{diversity_promotion}). After some research, it became apparent that the ($\kappa$ + $\mu$)-elimination operator actually puts a lot of selective pressure. A k-tournament operator, in contrast, can mitigate this selective pressure, hence the ($\kappa$ + $\mu$)-elimination operator has been exchanged for the k-tournament operator.

To combine the k-tournament operator with the fitness sharing operator, Algorithm \ref{elimination_algorithm} has been used.

\begin{algorithm}
\caption{Elimination \cite{eiben_smith}}\label{elimination_algorithm}
\begin{algorithmic}
\State calculate the fitnesses for all the individuals
\ % TODO first maybe finalize code
\end{algorithmic}
\end{algorithm}

% Value of k

\subsection{Local search operators}
\label{local_search_operator}
\ReplaceMe{What local search operators did you implement? Describe them. Did they cause a significant improvement in the performance of your algorithm? Why (not)? Did you consider other local search operators that did not make the cut? Why did you discard them? Are there parameters that need to be determined in your operator? Do you use an advanced scheme to determine them (e.g., adaptive or self-adaptive)?}

The 2-opt local search operator has been implemented, which swaps every two possible edges in a given cycle. In a first version of this algorithm, the fitness was recalculated for every possible `neighbor' of the given individual, which entailed an unacceptable high computational cost, especially for the larger problem sizes. After some investigation, patterns were detected in the computation of the fitness. Hence, instead of recalculating the fitness for every neighbor, some kind of dynamic programming approach was undertaken. For every individual, there is a sort of preprocessing step, whereby so-called `cumulative' are created. These cumulatives capture the path length from the first city to that corresponding city in the cumulative array. The same process applies for the calculation of the path length from the last city to the corresponding city in the array (i.e. in reverse order, whereby the return cost of the last city to the first city is also incorporated). It is clear that the calculation of these cumulatives is done in $O(N)$, where $N$ is the number of cities in the problem size.

Now, calculations of fitnesses of individuals are simply a matter of bookkeeping. The process is explained in Algorithm \ref{local_search_operator_algorithm}.

% TODO maybe explain the loops a bit in words as well

\begin{algorithm}
\caption{Local search operator}\label{local_search_operator_algorithm}
\begin{algorithmic}
\State Let the best fitness be the fitness of the original individual
\State Let the best combination be (0, 0)
\State Build cumulatives
\For {first from 1 to (length - 3)} 
\State $fit_{first \; part} = forward\_cumulative[first - 1]$
\State $fit_{middle \; part} = 0.0$
\For {second from (first + 2) to (length - 1)}
\State $fit_{middle \; part} \mathrel{{+}{=}} distanceMatrix[order[second-1]][order[second-2]]$
\State $fit_{last \; part} = backward\_cumulative[second]$
\State $first\_bridge = distanceMatrix[order[first-1]][order[second-1]]$
\State $second\_bridge = distanceMatrix[order[first]][order[second]]$
\State $fitness = fit_{first \; part} + first\_bridge + fit_{middle \; part} + second\_bridge + fit_{last \; part}$
\If {$fitness < best\_fitness$}
\State Let the new best combination be (first, second)
\State Let the new best fitness be the newly calculated fitness
\EndIf
\EndFor
\EndFor
\State Swap the order of the individual from from the best first and best second, obtained from the best combination.
\end{algorithmic}
\end{algorithm}

It should also be noted that by using Numba with the command $@jit(nopython=True)$ above the method declarations, the local search operator runs \textbf{745 times as fast}. Numba can make these huge improvements due to compilation of these methods, where especially the looping contributes massively to the speedup.

% TODO maybe some kind of illustration

\subsection{Diversity promotion mechanisms}
\label{diversity_promotion}
\ReplaceMe{Did you implement a diversity promotion scheme? If yes, which one? If no, why not? Describe the mechanism you implemented. In what sense does the mechanism improve the performance of your evolutionary algorithm? Are there parameters that need to be determined? Did you use an advanced scheme to determine them?}

\subsection{Stopping criterion}
Not a lot of effort has been put in implementing a stopping criterion, since all the larger problems stayed converging after running for five minutes. Hence, the stopping criterion is simply the time limit of five minutes.

% TODO maybe some other stopping criterion?
\ReplaceMe{Which stopping criterion did you implement? Did you combine several criteria?}

\subsection{Parameter selection}

\ReplaceMe{For all of the parameters that are not automatically determined by adaptivity or self-adaptivity (as you have described above), describe how you determined them. Did you perform a hyperparameter search? How did you do this? How did you determine these parameters would be valid both for small and large problem instances?}

\subsection{Other considerations}

% TODO hashmaps for fitnesses
% Elitism

\ReplaceMe{Did you consider other items not listed above, such as elitism, multiobjective optimization strategies (e.g., island model, pareto front approximation), a parallel implementation, or other interesting computational optimizations (e.g. using advanced algorithms or data structures)? You can describe them here or add additional subsections as needed.}


\section{Numerical experiments (target: 1.5 pages)}

\RemoveMe{\textbf{Goal:} Based on this section and our execution of your code, we will evaluate the performance (time, quality of solutions) of your implementation and your ability to interpret and explain the results on benchmark problems.}

\subsection{Metadata}

\ReplaceMe{What parameters are there to choose in your evolutionary algorithm? Which fixed parameter values did you use for all experiments below? If some parameters are determined based on information from the problem instance (e.g., number of cities), also report their specific values for the problems below.

Report the main characteristics of the computer system on which you ran your evolutionary algorithm. Include the processor or CPU (including the number of cores and clock speed), the amount of main memory, and the version of Python 3.}
These experiments were conducted on an Intel Core i7-6700HQ CPU, with a clock frequency of 3.60GHz and 8 virtual cores. The systems contains 16 GB of main memory, and Python version 3.8 was used for the tests.

\subsection{tour29.csv}

\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). Include a typical convergence graph, by plotting the mean and best objective values in function of the time (for example based on the output of the Reporter class). 

What is the best tour length you found? What is the corresponding sequence of cities? 

Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one?

Solve this problem 1000 times and record the results. Make a histogram of the final mean fitnessess and the final best fitnesses of the 1000 runs. Comment on this figure: is there a lot of variability in the results, what are the means and the standard deviations?}

\subsection{tour100.csv}

\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). Include a typical convergence graph, by plotting the mean and best objective values in function of the time (for example based on the output of the Reporter class). 

What is the best tour length you found in each case? 

Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one?}

\subsection{tour500.csv}

\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). Include a typical convergence graph, by plotting the mean and best objective values in function of the time (for example based on the output of the Reporter class). 

What is the best tour length you found? 

Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one?}

\subsection{tour1000.csv}

\ReplaceMe{Run your algorithm on this benchmark problem (with the 5 minute time limit from the Reporter). Include a typical convergence graph, by plotting the mean and best objective values in function of the time (for example based on the output of the Reporter class). 

What is the best tour length you found? 

Interpret your results. How do you rate the performance of your algorithm (time, memory, speed of convergence, diversity of population, quality of the best solution, etc)? Is your solution close to the optimal one? }


\section{Critical reflection (target: 0.75 pages)}

\RemoveMe{\textbf{Goal:} Based on this section, we will evaluate your understanding and insight into the main strengths and weaknesses of your evolutionary algorithms.}

\ReplaceMe{What are the three main strengths of evolutionary algorithms in your experience?}

\begin{enumerate}
 \item By using a population-based metaheuristic, a tradeoff can be made between exploration and exploitation. This tradeoff turned out to be instrumental in finding good solutions for the travelling salesman problem. Pure random search is computationally way too expensive (it would even take an infinity before a `decent', simply greedy solution would have been found). Pure local optimizers, on the other hand, would quickly converge to one suboptimal solution, which would most likely also be relatively far from the most optimal solution. Evolutionary algorithms provide a way to find good sub-optimal solutions in a decent amount of time.
 \item Evolutionary algorithms are in principle relatively easy to parallelize, since the population can explore the search space concurrently in many directions. Although in this project, not a lot of parallelization had been used (only in the initialization) step, it would probably be the next most beneficial task to undertake. Trying to use multiprocessing has been undertaken, but the execution time when using multiprocessing was more than a thousand times slower, compared to simple sequential execution on one CPU core. This probably came due to a lot of interprocess communication (IPC), since for example the list of individuals was shared, which resulted in a lot of locks and semiphores, that clearly dominated the execution time.
 \item 
\end{enumerate}

\ReplaceMe{What are the three main weak points of evolutionary algorithms in your experience?}

\begin{enumerate}
 \item % Must be carefully designed
 \item 
 \item 
\end{enumerate}

\ReplaceMe{Describe the main lessons learned from this project. Do you believe evolutionary algorithms are appropriate for the problem studied in this project? Why (not)? What surprised you and why? What did you learn from this project?}

% Numba!

\section{Other comments} \label{sec_other}

\ReplaceMe{In case you think there is something important to discuss that is not covered by the previous sections, you can do it here. }

\bibliographystyle{plain} 
\bibliography{genetic_algorithms_project}

\end{document}
